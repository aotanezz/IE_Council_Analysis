# -*- coding: utf-8 -*-
"""LDA_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWa2xGyoO8cULHDWqJzbi4pH7a3VILf_

# The Politics of Warehousing in the Inland Empire, CA: How did we get here?
## Latent Dirichlet Allocation (LDA)
### By: Alyson Ota√±ez

## Exploratory Analysis

# 1. Setup
"""

# Install packages if necessary
# ! pip install pandas
# ! pip install re
# ! pip install matplotlib
# ! pip install seaborn
# ! pip install warnings

#! pip install --force-reinstall pyldavis

# Import necessary packages
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
import os

import pyLDAvis
import pyLDAvis.gensim
import pyLDAvis.gensim_models as gensimvis

# Load Google Drive
drive.mount('/content/drive')

# Load data
ie_cities = pd.read_csv('/content/drive/My Drive/Data/ie_cities.csv')

# Drop NA values (only 1)
ie_cities = ie_cities[ie_cities['Text'].notna()]

# Remove missing years
ie_cities = ie_cities[~ie_cities['Year'].isin([2029, 0])]

# View data
ie_cities.head()

"""# 2. Keywords Over Time"""

# Keywords
industrial = ['warehouse', 'logistics', 'distribution', 'shipping',
                'industrial', 'warehousing', 'diesel', 'freight', 'cargo',
                'zoning', 'zone']

housing =['home', 'house', 'apartment', 'townhouse', 'condominium',
             'housing', 'rent', 'mortgage', 'residential', 'homeless',
             'homelessness']

jobs = ['job', 'economy', 'workforce', 'employment', 'wages', 'trade',
          'salary', 'workers', 'unemployment', 'jobs', 'employer']

safety = ['police', 'safety', 'crime', 'security', 'firefighter', 'drugs',
            'theft', 'gun', 'violence', 'paramedic', 'policing']

# Plot of industrial terms over time
def count_industrial(text):
    words = re.split(r'\s+', text.lower())
    return sum(word in industrial for word in words)

# Apply the function to each text entry and create a new column for sums
ie_cities['Sum_Industrial'] = ie_cities['Text'].apply(count_industrial)

# Group by Year and sum the counts
yearly_counts_in = ie_cities.groupby('Year')['Sum_Industrial'].sum().reset_index()

# Plotting
plt.figure(figsize=(15, 6))
sns.barplot(data=yearly_counts_in, x='Year', y='Sum_Industrial', color='#ff9998')
plt.xlabel('Year')
plt.ylabel('Count of Industrial Terms')
plt.title('Sum of Mention of Industrial Terms in Inland Empire, CA')
plt.savefig('industrial_terms_plot.png')
plt.show()

# Plot of housing terms over time
def count_housing(text):
    words = re.split(r'\s+', text.lower())
    return sum(word in housing for word in words)

# Apply the function to each text entry and create a new column for sums
ie_cities['Sum_Housing'] = ie_cities['Text'].apply(count_housing)

# Group by Year and sum the counts
yearly_counts_in = ie_cities.groupby('Year')['Sum_Housing'].sum().reset_index()

# Plotting
plt.figure(figsize=(15, 6))
sns.barplot(data=yearly_counts_in, x='Year', y='Sum_Housing', color='#9acbff')
plt.xlabel('Year')
plt.ylabel('Count of Housing Terms')
plt.title('Sum of Mention of Housing Terms in Inland Empire, CA')
plt.savefig('housing_terms_plot.png')
plt.show()

# Plot of jobs terms over time
def count_jobs(text):
    words = re.split(r'\s+', text.lower())
    return sum(word in jobs for word in words)

# Apply the function to each text entry and create a new column for sums
ie_cities['Sum_Jobs'] = ie_cities['Text'].apply(count_jobs)

# Group by Year and sum the counts
yearly_counts_in = ie_cities.groupby('Year')['Sum_Jobs'].sum().reset_index()

# Plotting
plt.figure(figsize=(15, 6))
sns.barplot(data=yearly_counts_in, x='Year', y='Sum_Jobs', color='#049d73')
plt.xlabel('Year')
plt.ylabel('Count of Jobs Terms')
plt.title('Sum of Mention of Job Terms in Inland Empire, CA')
plt.savefig('jobs_terms_plot.png')
plt.show()

# Plot of safety terms over time
def count_safety(text):
    words = re.split(r'\s+', text.lower())
    return sum(word in safety for word in words)

# Apply the function to each text entry and create a new column for sums
ie_cities['Sum_Safety'] = ie_cities['Text'].apply(count_safety)

# Group by Year and sum the counts
yearly_counts_in = ie_cities.groupby('Year')['Sum_Safety'].sum().reset_index()

# Plotting
plt.figure(figsize=(15, 6))
sns.barplot(data=yearly_counts_in, x='Year', y='Sum_Safety', color='#e59f00')
plt.xlabel('Year')
plt.ylabel('Count of Safety Terms')
plt.title('Sum of Mention of Safety Terms in Inland Empire, CA')
plt.savefig('safety_terms_plot.png')
plt.show()

# Combined plot of word count over time

# Group by year and sum the counts
yearly_counts = ie_cities.groupby('Year')[['Sum_Industrial', 'Sum_Housing', 'Sum_Jobs', 'Sum_Safety']].sum().reset_index()

# Melt dataframe to long format for plotting
yearly_counts_melted = yearly_counts.melt(id_vars='Year',
                                          value_vars=['Sum_Industrial', 'Sum_Housing', 'Sum_Jobs', 'Sum_Safety'],
                                          var_name='Category',
                                          value_name='Count')

"""# Latent Dirichlet Allocation (LDA)

# 1. Setup
"""

# Install packages if necessary
# ! pip install nltk
# ! pip install spacy
# ! pip install gensim
# ! pip install pyLDAvis
# ! pip install gutenbergpy

# Import necessary packages
import pandas as pd
import os
import nltk
import re
import string
import sys
import gensim
import numpy as np
from nltk.tokenize import word_tokenize
nltk.download("punkt")
nltk.download('wordnet')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel

"""# 2. Pre-process data"""

# WordNet for lemmatization
def wordnet_pos_tags(x):
    if x.startswith('J'):
        return wordnet.ADJ
    elif x.startswith('V'):
        return wordnet.VERB
    elif x.startswith('N'):
        return wordnet.NOUN
    elif x.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

# Function for preprocessing
def txt_preprocess_pipeline(text):
    standard_txt = text.lower()

    clean_txt = re.sub(r'http\S+|www\S+|https\S+', '', standard_txt, flags = re.MULTILINE)
    clean_txt = re.sub(r'\n', ' ', clean_txt)
    clean_txt = re.sub(r'\s+', ' ', clean_txt)
    clean_txt = re.sub(r'\S+@\S+', '', clean_txt)
    clean_txt = re.sub(r'\\r\\n', ' ', clean_txt)
    clean_txt = re.sub(r'\s+', ' ', clean_txt)
    clean_txt = re.sub(r'<.*?>', '', clean_txt)
    clean_txt = re.sub(r'[^\w\s]', '', clean_txt)
    clean_txt = re.sub(r'\b\w{1,2}\b', '', clean_txt)

    tokens = word_tokenize(clean_txt)
    filtered_tokens_alpha = [word for word in tokens if word.isalpha() and not re.match(r'^[ivxlcdm]+$', word)]

    stop_words = set(stopwords.words('english'))
    stop_words.update(['shall', 'chino', 'fontana', 'march', 'joint', 'powers', 'authority',
                       'http', 'rialto', 'ontario', 'city', 'council', 'agenda',
                      'meeting', 'minutes', 'back', 'site', 'main', 'welcome', 'browse', 'video',
                      'monday', 'tuesday', 'wednesday', 'thursday', 'friday',
                      'saturday', 'sunday', 'notice', 'commission', 'archive', 'pmcity',
                      'chamber', 'palm', 'ave', 'january', 'february', 'march', 'april', 'may',
                      'june', 'july', 'august', 'september', 'october', 'november', 'december',
                      'closed', 'session'])
    filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]

    lemmatizer = WordNetLemmatizer()
    pos_tags = nltk.pos_tag(filtered_tokens_final)
    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]

    return lemma_tokens

# Apply functions to data
ie_cities['Processed_Text'] = ie_cities['Text'].apply(txt_preprocess_pipeline)
ie_cities.head()

"""# 3. LDA Model"""

# Load dictionary
dictionary_corpus = corpora.Dictionary(ie_cities['Processed_Text'])
dictionary_corpus.filter_extremes(no_below=20, no_above=0.5)

# Generate corpus as BoW
corpus = [dictionary_corpus.doc2bow(i) for i in  ie_cities['Processed_Text']]

# Train LDA model
lda_model_corpus = LdaModel(corpus = corpus, id2word = dictionary_corpus, random_state = 4583,
                     chunksize = 20, num_topics = 6, passes = 200, iterations= 400)

# Print LDA topics
for idx, topic in lda_model_corpus.print_topics(num_topics = 6, num_words =10):
    print(f"Topic {idx+1}: {topic}")

# Visualization
dickens_visual_corpus = pyLDAvis.gensim.prepare(lda_model_corpus, corpus, dictionary_corpus, mds='mmds')
pyLDAvis.save_html(dickens_visual_corpus, 'lda_visualization.html')

# Plot
pyLDAvis.display(dickens_visual_corpus)